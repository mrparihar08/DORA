{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a190894",
   "metadata": {},
   "source": [
    "# Simple Disease CSV â€” Analysis & Model Building\n",
    "Is notebook me hum dataset.csv load karenge, symptoms ko combine karenge, ek chhota RandomForest model train karenge aur artifacts save karenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7848139d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "BASE_DIR = Path('.').resolve()\n",
    "DATA_PATH = BASE_DIR / 'dataset.csv'   # dataset.csv ko notebook folder me rakho\n",
    "ARTIFACTS_DIR = BASE_DIR / 'artifacts'\n",
    "ARTIFACTS_DIR.mkdir(exist_ok=True)\n",
    "print(\"DATA_PATH =\", DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d17180",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "if not DATA_PATH.exists():\n",
    "    raise FileNotFoundError(f\"dataset.csv nahi mila: {DATA_PATH}\")\n",
    "\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "    df.columns = [c.strip().replace(' ', '_') for c in df.columns]\n",
    "    print(\"Shape:\", df.shape)\n",
    "    display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fcc620",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# CSV me symptom wale columns ka naam symptom_1, symptom_2 ... jo bhi hai uske hisab se detect karega\n",
    "symptom_cols = [c for c in df.columns if 'symptom' in c.lower()]\n",
    "print(\"Found symptom cols:\", symptom_cols)\n",
    "\n",
    "# agar mil gaye to join kar do, warna ek error bathao\n",
    "if not symptom_cols:\n",
    "    raise RuntimeError(\"Koi symptom_* column nahi mila. CSV ke columns check karo.\")\n",
    "\n",
    "    df_sym = df[symptom_cols].fillna('').astype(str)\n",
    "    df['symptom_text'] = df_sym.apply(lambda row: ' '.join(row.str.strip().str.lower()), axis=1)\n",
    "    display(df[['symptom_text']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26aab62",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Target: 'Disease' column use karo\n",
    "if 'Disease' not in df.columns:\n",
    "    raise RuntimeError(\"CSV me 'Disease' column hona chahiye.\")\n",
    "\n",
    "    # Rare labels ko 'Other' me group karna (optional, simple rule)\n",
    "    min_samples = 2\n",
    "    vc = df['Disease'].value_counts()\n",
    "    rare = vc[vc < min_samples].index.tolist()\n",
    "    df['Disease_clean'] = df['Disease'].apply(lambda x: x if x not in rare else 'Other')\n",
    "\n",
    "    # TF-IDF\n",
    "    vectorizer = TfidfVectorizer(max_features=2000, stop_words='english')\n",
    "    X = vectorizer.fit_transform(df['symptom_text'].fillna(''))\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(df['Disease_clean'])\n",
    "\n",
    "    print(\"X shape:\", X.shape)\n",
    "    print(\"Classes:\", list(le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041e7f52",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Test accuracy:\", round(accuracy_score(y_test, y_pred), 4))\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=le.inverse_transform(sorted(set(y_test)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9498b336",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "joblib.dump(model, ARTIFACTS_DIR / 'disease_model.pkl', compress=('gzip',3))\n",
    "joblib.dump(vectorizer, ARTIFACTS_DIR / 'tfidf_vectorizer.pkl', compress=('gzip',3))\n",
    "joblib.dump(le, ARTIFACTS_DIR / 'label_encoder.pkl', compress=('gzip',3))\n",
    "print(\"Saved artifacts to\", ARTIFACTS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b07251f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Sample symptoms (aap change kar sakte ho)\n",
    "sample = [\"fever\", \"cough\", \"sore throat\"]\n",
    "input_text = \" \".join([s.lower() for s in sample])\n",
    "X_in = vectorizer.transform([input_text])\n",
    "pred_idx = model.predict(X_in)[0]\n",
    "pred_label = le.inverse_transform([pred_idx])[0]\n",
    "print(\"Input:\", sample)\n",
    "print(\"Predicted disease:\", pred_label)\n",
    "\n",
    "# Agar model.predict_proba chahiye:\n",
    "if hasattr(model, \"predict_proba\"):\n",
    "    probs = model.predict_proba(X_in)[0]\n",
    "        print(\"Confidence (predicted class prob):\", round(probs[pred_idx], 3))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
